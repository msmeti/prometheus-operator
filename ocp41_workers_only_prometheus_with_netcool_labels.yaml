apiVersion: v1
items:
- apiVersion: monitoring.coreos.com/v1
  kind: PrometheusRule
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"monitoring.coreos.com/v1","kind":"PrometheusRule","metadata":{"annotations":{},"creationTimestamp":"2019-08-07T09:52:47Z","generation":1,"labels":{"app":"prometheus","chart":"kube-prometheus-0.0.105","heritage":"Tiller","prometheus":"kube-prometheus","release":"kube-prometheus"},"name":"kube-prometheus","namespace":"monitoring","resourceVersion":"101982","selfLink":"/apis/monitoring.coreos.com/v1/namespaces/monitoring/prometheusrules/kube-prometheus","uid":"1c718fc4-b8f9-11e9-be96-f25fe3364e4a"},"spec":{"groups":[{"name":"general.rules","rules":[{"alert":"TargetDown","annotations":{"description":"{{ $value }}% of {{ $labels.job }} targets are down.","summary":"Targets are down"},"expr":"100 * (count(up == 0) BY (job) / count(up) BY (job)) \u003e 10","for":"10m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"DeadMansSwitch","annotations":{"description":"This is a DeadMansSwitch meant to ensure that the entire Alerting pipeline is functional.","summary":"Alerting DeadMansSwitch"},"expr":"vector(1)","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"none"}},{"expr":"process_open_fds / process_max_fds","record":"fd_utilization"},{"alert":"FdExhaustionClose","annotations":{"description":"{{ $labels.job }}: {{ $labels.namespace }}/{{ $labels.pod }} instance will exhaust in file/socket descriptors within the next 4 hours","summary":"file descriptors soon exhausted"},"expr":"predict_linear(fd_utilization[1h], 3600 * 4) \u003e 1","for":"10m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"FdExhaustionClose","annotations":{"description":"{{ $labels.job }}: {{ $labels.namespace }}/{{ $labels.pod }} instance will exhaust in file/socket descriptors within the next hour","summary":"file descriptors soon exhausted"},"expr":"predict_linear(fd_utilization[10m], 3600) \u003e 1","for":"10m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}}]}]}}
    creationTimestamp: "2019-08-07T09:52:47Z"
    generation: 1
    labels:
      app: prometheus
      chart: kube-prometheus-0.0.105
      heritage: Tiller
      prometheus: kube-prometheus
      release: kube-prometheus
    name: kube-prometheus
    namespace: monitoring
    resourceVersion: "13922855"
    selfLink: /apis/monitoring.coreos.com/v1/namespaces/monitoring/prometheusrules/kube-prometheus
    uid: 1c718fc4-b8f9-11e9-be96-f25fe3364e4a
  spec:
    groups:
    - name: general.rules
      rules:
      - alert: TargetDown
        annotations:
          description: '{{ $value }}% of {{ $labels.job }} targets are down.'
          summary: Targets are down
        expr: 100 * (count(up == 0) BY (job) / count(up) BY (job)) > 10
        for: 10m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: DeadMansSwitch
        annotations:
          description: This is a DeadMansSwitch meant to ensure that the entire Alerting
            pipeline is functional.
          summary: Alerting DeadMansSwitch
        expr: vector(1)
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: none
      - expr: process_open_fds / process_max_fds
        record: fd_utilization
      - alert: FdExhaustionClose
        annotations:
          description: '{{ $labels.job }}: {{ $labels.namespace }}/{{ $labels.pod
            }} instance will exhaust in file/socket descriptors within the next 4
            hours'
          summary: file descriptors soon exhausted
        expr: predict_linear(fd_utilization[1h], 3600 * 4) > 1
        for: 10m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: FdExhaustionClose
        annotations:
          description: '{{ $labels.job }}: {{ $labels.namespace }}/{{ $labels.pod
            }} instance will exhaust in file/socket descriptors within the next hour'
          summary: file descriptors soon exhausted
        expr: predict_linear(fd_utilization[10m], 3600) > 1
        for: 10m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
- apiVersion: monitoring.coreos.com/v1
  kind: PrometheusRule
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"monitoring.coreos.com/v1","kind":"PrometheusRule","metadata":{"annotations":{},"creationTimestamp":"2019-08-07T09:52:47Z","generation":1,"labels":{"app":"alertmanager","chart":"alertmanager-0.1.7","heritage":"Tiller","prometheus":"kube-prometheus","release":"kube-prometheus"},"name":"kube-prometheus-alertmanager","namespace":"monitoring","resourceVersion":"101947","selfLink":"/apis/monitoring.coreos.com/v1/namespaces/monitoring/prometheusrules/kube-prometheus-alertmanager","uid":"1c468e86-b8f9-11e9-be96-f25fe3364e4a"},"spec":{"groups":[{"name":"alertmanager.rules","rules":[{"alert":"AlertmanagerConfigInconsistent","annotations":{"description":"The configuration of the instances of the Alertmanager cluster `{{$labels.service}}` are out of sync.","summary":"Configuration out of sync"},"expr":"count_values(\"config_hash\", alertmanager_config_hash) BY (service) / ON(service) GROUP_LEFT() label_replace(prometheus_operator_alertmanager_spec_replicas, \"service\", \"alertmanager-$1\", \"alertmanager\", \"(.*)\") != 1","for":"5m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}},{"alert":"AlertmanagerDownOrMissing","annotations":{"description":"An unexpected number of Alertmanagers are scraped or Alertmanagers disappeared from discovery.","summary":"Alertmanager down or missing"},"expr":"label_replace(prometheus_operator_alertmanager_spec_replicas, \"job\", \"alertmanager-$1\", \"alertmanager\", \"(.*)\") / ON(job) GROUP_RIGHT() sum(up) BY (job) != 1","for":"5m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"AlertmanagerFailedReload","annotations":{"description":"Reloading Alertmanager's configuration has failed for {{ $labels.namespace }}/{{ $labels.pod}}.","summary":"Alertmanager's configuration reload failed"},"expr":"alertmanager_config_last_reload_successful == 0","for":"10m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}}]}]}}
    creationTimestamp: "2019-08-07T09:52:47Z"
    generation: 1
    labels:
      app: alertmanager
      chart: alertmanager-0.1.7
      heritage: Tiller
      prometheus: kube-prometheus
      release: kube-prometheus
    name: kube-prometheus-alertmanager
    namespace: monitoring
    resourceVersion: "13922857"
    selfLink: /apis/monitoring.coreos.com/v1/namespaces/monitoring/prometheusrules/kube-prometheus-alertmanager
    uid: 1c468e86-b8f9-11e9-be96-f25fe3364e4a
  spec:
    groups:
    - name: alertmanager.rules
      rules:
      - alert: AlertmanagerConfigInconsistent
        annotations:
          description: The configuration of the instances of the Alertmanager cluster
            `{{$labels.service}}` are out of sync.
          summary: Configuration out of sync
        expr: count_values("config_hash", alertmanager_config_hash) BY (service) /
          ON(service) GROUP_LEFT() label_replace(prometheus_operator_alertmanager_spec_replicas,
          "service", "alertmanager-$1", "alertmanager", "(.*)") != 1
        for: 5m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
      - alert: AlertmanagerDownOrMissing
        annotations:
          description: An unexpected number of Alertmanagers are scraped or Alertmanagers
            disappeared from discovery.
          summary: Alertmanager down or missing
        expr: label_replace(prometheus_operator_alertmanager_spec_replicas, "job",
          "alertmanager-$1", "alertmanager", "(.*)") / ON(job) GROUP_RIGHT() sum(up)
          BY (job) != 1
        for: 5m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: AlertmanagerFailedReload
        annotations:
          description: Reloading Alertmanager's configuration has failed for {{ $labels.namespace
            }}/{{ $labels.pod}}.
          summary: Alertmanager's configuration reload failed
        expr: alertmanager_config_last_reload_successful == 0
        for: 10m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
- apiVersion: monitoring.coreos.com/v1
  kind: PrometheusRule
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"monitoring.coreos.com/v1","kind":"PrometheusRule","metadata":{"annotations":{},"creationTimestamp":"2019-08-07T09:52:47Z","generation":1,"labels":{"app":"prometheus","chart":"exporter-kube-controller-manager-0.1.10","heritage":"Tiller","prometheus":"kube-prometheus","release":"kube-prometheus"},"name":"kube-prometheus-exporter-kube-controller-manager","namespace":"monitoring","resourceVersion":"101955","selfLink":"/apis/monitoring.coreos.com/v1/namespaces/monitoring/prometheusrules/kube-prometheus-exporter-kube-controller-manager","uid":"1c4bf239-b8f9-11e9-be96-f25fe3364e4a"},"spec":{"groups":[{"name":"kube-controller-manager.rules","rules":[{"alert":"K8SControllerManagerDown","annotations":{"description":"There is no running K8S controller manager. Deployments and replication controllers are not making progress.","runbook":"https://coreos.com/tectonic/docs/latest/troubleshooting/controller-recovery.html#recovering-a-controller-manager","summary":"Controller manager is down"},"expr":"absent(up{job=\"kube-controller-manager\"} == 1)","for":"5m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}}]}]}}
    creationTimestamp: "2019-08-07T09:52:47Z"
    generation: 1
    labels:
      app: prometheus
      chart: exporter-kube-controller-manager-0.1.10
      heritage: Tiller
      prometheus: kube-prometheus
      release: kube-prometheus
    name: kube-prometheus-exporter-kube-controller-manager
    namespace: monitoring
    resourceVersion: "13922860"
    selfLink: /apis/monitoring.coreos.com/v1/namespaces/monitoring/prometheusrules/kube-prometheus-exporter-kube-controller-manager
    uid: 1c4bf239-b8f9-11e9-be96-f25fe3364e4a
  spec:
    groups:
    - name: kube-controller-manager.rules
      rules:
      - alert: K8SControllerManagerDown
        annotations:
          description: There is no running K8S controller manager. Deployments and
            replication controllers are not making progress.
          runbook: https://coreos.com/tectonic/docs/latest/troubleshooting/controller-recovery.html#recovering-a-controller-manager
          summary: Controller manager is down
        expr: absent(up{job="kube-controller-manager"} == 1)
        for: 5m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
- apiVersion: monitoring.coreos.com/v1
  kind: PrometheusRule
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"monitoring.coreos.com/v1","kind":"PrometheusRule","metadata":{"annotations":{},"creationTimestamp":"2019-08-07T09:52:47Z","generation":1,"labels":{"app":"prometheus","chart":"exporter-kube-etcd-0.1.15","heritage":"Tiller","prometheus":"kube-prometheus","release":"kube-prometheus"},"name":"kube-prometheus-exporter-kube-etcd","namespace":"monitoring","resourceVersion":"101956","selfLink":"/apis/monitoring.coreos.com/v1/namespaces/monitoring/prometheusrules/kube-prometheus-exporter-kube-etcd","uid":"1c5076cc-b8f9-11e9-be96-f25fe3364e4a"},"spec":{"groups":[{"name":"./etcd3.rules","rules":[{"alert":"InsufficientMembers","annotations":{"description":"If one more etcd member goes down the cluster will be unavailable","summary":"etcd cluster insufficient members"},"expr":"count(up{job=\"kube-etcd\"} == 0) \u003e (count(up{job=\"kube-etcd\"}) / 2 - 1)","for":"3m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}},{"alert":"NoLeader","annotations":{"description":"etcd member {{ $labels.instance }} has no leader","summary":"etcd member has no leader"},"expr":"etcd_server_has_leader{job=\"kube-etcd\"} == 0","for":"1m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}},{"alert":"HighNumberOfLeaderChanges","annotations":{"description":"etcd instance {{ $labels.instance }} has seen {{ $value }} leader changes within the last hour","summary":"a high number of leader changes within the etcd cluster are happening"},"expr":"increase(etcd_server_leader_changes_seen_total{job=\"kube-etcd\"}[1h]) \u003e 3","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"HighNumberOfFailedGRPCRequests","annotations":{"description":"{{ $value }}% of requests for {{ $labels.grpc_method }} failed on etcd instance {{ $labels.instance }}","summary":"a high number of gRPC requests are failing"},"expr":"sum(rate(grpc_server_handled_total{grpc_code!=\"OK\",job=\"kube-etcd\"}[5m])) BY (grpc_service, grpc_method) / sum(rate(grpc_server_handled_total{job=\"kube-etcd\"}[5m])) BY (grpc_service, grpc_method) \u003e 0.01","for":"10m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"HighNumberOfFailedGRPCRequests","annotations":{"description":"{{ $value }}% of requests for {{ $labels.grpc_method }} failed on etcd instance {{ $labels.instance }}","summary":"a high number of gRPC requests are failing"},"expr":"sum(rate(grpc_server_handled_total{grpc_code!=\"OK\",job=\"kube-etcd\"}[5m])) BY (grpc_service, grpc_method) / sum(rate(grpc_server_handled_total{job=\"kube-etcd\"}[5m])) BY (grpc_service, grpc_method) \u003e 0.05","for":"5m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}},{"alert":"GRPCRequestsSlow","annotations":{"description":"on etcd instance {{ $labels.instance }} gRPC requests to {{ $labels.grpc_method }} are slow","summary":"slow gRPC requests"},"expr":"histogram_quantile(0.99, sum(rate(grpc_server_handling_seconds_bucket{job=\"kube-etcd\",grpc_type=\"unary\"}[5m])) by (grpc_service, grpc_method, le)) \u003e 0.15","for":"10m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}},{"alert":"HighNumberOfFailedHTTPRequests","annotations":{"description":"{{ $value }}% of requests for {{ $labels.method }} failed on etcd instance {{ $labels.instance }}","summary":"a high number of HTTP requests are failing"},"expr":"sum(rate(etcd_http_failed_total{job=\"kube-etcd\"}[5m])) BY (method) / sum(rate(etcd_http_received_total{job=\"kube-etcd\"}[5m])) BY (method) \u003e 0.01","for":"10m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"HighNumberOfFailedHTTPRequests","annotations":{"description":"{{ $value }}% of requests for {{ $labels.method }} failed on etcd instance {{ $labels.instance }}","summary":"a high number of HTTP requests are failing"},"expr":"sum(rate(etcd_http_failed_total{job=\"kube-etcd\"}[5m])) BY (method) / sum(rate(etcd_http_received_total{job=\"kube-etcd\"}[5m])) BY (method) \u003e 0.05","for":"5m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}},{"alert":"HTTPRequestsSlow","annotations":{"description":"on etcd instance {{ $labels.instance }} HTTP requests to {{ $labels.method }} are slow","summary":"slow HTTP requests"},"expr":"histogram_quantile(0.99, rate(etcd_http_successful_duration_seconds_bucket[5m])) \u003e 0.15","for":"10m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"EtcdMemberCommunicationSlow","annotations":{"description":"etcd instance {{ $labels.instance }} member communication with {{ $labels.To }} is slow","summary":"etcd member communication is slow"},"expr":"histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket[5m])) \u003e 0.15","for":"10m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"HighNumberOfFailedProposals","annotations":{"description":"etcd instance {{ $labels.instance }} has seen {{ $value }} proposal failures within the last hour","summary":"a high number of proposals within the etcd cluster are failing"},"expr":"increase(etcd_server_proposals_failed_total{job=\"kube-etcd\"}[1h]) \u003e 5","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"HighFsyncDurations","annotations":{"description":"etcd instance {{ $labels.instance }} fync durations are high","summary":"high fsync durations"},"expr":"histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket[5m])) \u003e 0.5","for":"10m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"HighCommitDurations","annotations":{"description":"etcd instance {{ $labels.instance }} commit durations are high","summary":"high commit durations"},"expr":"histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket[5m])) \u003e 0.25","for":"10m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}}]}]}}
    creationTimestamp: "2019-08-07T09:52:47Z"
    generation: 1
    labels:
      app: prometheus
      chart: exporter-kube-etcd-0.1.15
      heritage: Tiller
      prometheus: kube-prometheus
      release: kube-prometheus
    name: kube-prometheus-exporter-kube-etcd
    namespace: monitoring
    resourceVersion: "13922865"
    selfLink: /apis/monitoring.coreos.com/v1/namespaces/monitoring/prometheusrules/kube-prometheus-exporter-kube-etcd
    uid: 1c5076cc-b8f9-11e9-be96-f25fe3364e4a
  spec:
    groups:
    - name: ./etcd3.rules
      rules:
      - alert: InsufficientMembers
        annotations:
          description: If one more etcd member goes down the cluster will be unavailable
          summary: etcd cluster insufficient members
        expr: count(up{job="kube-etcd"} == 0) > (count(up{job="kube-etcd"}) / 2 -
          1)
        for: 3m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
      - alert: NoLeader
        annotations:
          description: etcd member {{ $labels.instance }} has no leader
          summary: etcd member has no leader
        expr: etcd_server_has_leader{job="kube-etcd"} == 0
        for: 1m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
      - alert: HighNumberOfLeaderChanges
        annotations:
          description: etcd instance {{ $labels.instance }} has seen {{ $value }}
            leader changes within the last hour
          summary: a high number of leader changes within the etcd cluster are happening
        expr: increase(etcd_server_leader_changes_seen_total{job="kube-etcd"}[1h])
          > 3
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: HighNumberOfFailedGRPCRequests
        annotations:
          description: '{{ $value }}% of requests for {{ $labels.grpc_method }} failed
            on etcd instance {{ $labels.instance }}'
          summary: a high number of gRPC requests are failing
        expr: sum(rate(grpc_server_handled_total{grpc_code!="OK",job="kube-etcd"}[5m]))
          BY (grpc_service, grpc_method) / sum(rate(grpc_server_handled_total{job="kube-etcd"}[5m]))
          BY (grpc_service, grpc_method) > 0.01
        for: 10m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: HighNumberOfFailedGRPCRequests
        annotations:
          description: '{{ $value }}% of requests for {{ $labels.grpc_method }} failed
            on etcd instance {{ $labels.instance }}'
          summary: a high number of gRPC requests are failing
        expr: sum(rate(grpc_server_handled_total{grpc_code!="OK",job="kube-etcd"}[5m]))
          BY (grpc_service, grpc_method) / sum(rate(grpc_server_handled_total{job="kube-etcd"}[5m]))
          BY (grpc_service, grpc_method) > 0.05
        for: 5m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
      - alert: GRPCRequestsSlow
        annotations:
          description: on etcd instance {{ $labels.instance }} gRPC requests to {{
            $labels.grpc_method }} are slow
          summary: slow gRPC requests
        expr: histogram_quantile(0.99, sum(rate(grpc_server_handling_seconds_bucket{job="kube-etcd",grpc_type="unary"}[5m]))
          by (grpc_service, grpc_method, le)) > 0.15
        for: 10m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
      - alert: HighNumberOfFailedHTTPRequests
        annotations:
          description: '{{ $value }}% of requests for {{ $labels.method }} failed
            on etcd instance {{ $labels.instance }}'
          summary: a high number of HTTP requests are failing
        expr: sum(rate(etcd_http_failed_total{job="kube-etcd"}[5m])) BY (method) /
          sum(rate(etcd_http_received_total{job="kube-etcd"}[5m])) BY (method) > 0.01
        for: 10m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: HighNumberOfFailedHTTPRequests
        annotations:
          description: '{{ $value }}% of requests for {{ $labels.method }} failed
            on etcd instance {{ $labels.instance }}'
          summary: a high number of HTTP requests are failing
        expr: sum(rate(etcd_http_failed_total{job="kube-etcd"}[5m])) BY (method) /
          sum(rate(etcd_http_received_total{job="kube-etcd"}[5m])) BY (method) > 0.05
        for: 5m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
      - alert: HTTPRequestsSlow
        annotations:
          description: on etcd instance {{ $labels.instance }} HTTP requests to {{
            $labels.method }} are slow
          summary: slow HTTP requests
        expr: histogram_quantile(0.99, rate(etcd_http_successful_duration_seconds_bucket[5m]))
          > 0.15
        for: 10m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: EtcdMemberCommunicationSlow
        annotations:
          description: etcd instance {{ $labels.instance }} member communication with
            {{ $labels.To }} is slow
          summary: etcd member communication is slow
        expr: histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket[5m]))
          > 0.15
        for: 10m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: HighNumberOfFailedProposals
        annotations:
          description: etcd instance {{ $labels.instance }} has seen {{ $value }}
            proposal failures within the last hour
          summary: a high number of proposals within the etcd cluster are failing
        expr: increase(etcd_server_proposals_failed_total{job="kube-etcd"}[1h]) >
          5
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: HighFsyncDurations
        annotations:
          description: etcd instance {{ $labels.instance }} fync durations are high
          summary: high fsync durations
        expr: histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket[5m]))
          > 0.5
        for: 10m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: HighCommitDurations
        annotations:
          description: etcd instance {{ $labels.instance }} commit durations are high
          summary: high commit durations
        expr: histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket[5m]))
          > 0.25
        for: 10m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
- apiVersion: monitoring.coreos.com/v1
  kind: PrometheusRule
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"monitoring.coreos.com/v1","kind":"PrometheusRule","metadata":{"annotations":{},"creationTimestamp":"2019-08-07T09:52:47Z","generation":1,"labels":{"app":"prometheus","chart":"exporter-kube-scheduler-0.1.9","heritage":"Tiller","prometheus":"kube-prometheus","release":"kube-prometheus"},"name":"kube-prometheus-exporter-kube-scheduler","namespace":"monitoring","resourceVersion":"101960","selfLink":"/apis/monitoring.coreos.com/v1/namespaces/monitoring/prometheusrules/kube-prometheus-exporter-kube-scheduler","uid":"1c539d06-b8f9-11e9-be96-f25fe3364e4a"},"spec":{"groups":[{"name":"kube-scheduler.rules","rules":[{"expr":"histogram_quantile(0.99, sum(scheduler_e2e_scheduling_latency_microseconds_bucket) BY (le, cluster)) / 1e+06","labels":{"quantile":"0.99"},"record":"cluster:scheduler_e2e_scheduling_latency_seconds:quantile"},{"expr":"histogram_quantile(0.9, sum(scheduler_e2e_scheduling_latency_microseconds_bucket) BY (le, cluster)) / 1e+06","labels":{"quantile":"0.9"},"record":"cluster:scheduler_e2e_scheduling_latency_seconds:quantile"},{"expr":"histogram_quantile(0.5, sum(scheduler_e2e_scheduling_latency_microseconds_bucket) BY (le, cluster)) / 1e+06","labels":{"quantile":"0.5"},"record":"cluster:scheduler_e2e_scheduling_latency_seconds:quantile"},{"expr":"histogram_quantile(0.99, sum(scheduler_scheduling_algorithm_latency_microseconds_bucket) BY (le, cluster)) / 1e+06","labels":{"quantile":"0.99"},"record":"cluster:scheduler_scheduling_algorithm_latency_seconds:quantile"},{"expr":"histogram_quantile(0.9, sum(scheduler_scheduling_algorithm_latency_microseconds_bucket) BY (le, cluster)) / 1e+06","labels":{"quantile":"0.9"},"record":"cluster:scheduler_scheduling_algorithm_latency_seconds:quantile"},{"expr":"histogram_quantile(0.5, sum(scheduler_scheduling_algorithm_latency_microseconds_bucket) BY (le, cluster)) / 1e+06","labels":{"quantile":"0.5"},"record":"cluster:scheduler_scheduling_algorithm_latency_seconds:quantile"},{"expr":"histogram_quantile(0.99, sum(scheduler_binding_latency_microseconds_bucket) BY (le, cluster)) / 1e+06","labels":{"quantile":"0.99"},"record":"cluster:scheduler_binding_latency_seconds:quantile"},{"expr":"histogram_quantile(0.9, sum(scheduler_binding_latency_microseconds_bucket) BY (le, cluster)) / 1e+06","labels":{"quantile":"0.9"},"record":"cluster:scheduler_binding_latency_seconds:quantile"},{"expr":"histogram_quantile(0.5, sum(scheduler_binding_latency_microseconds_bucket) BY (le, cluster)) / 1e+06","labels":{"quantile":"0.5"},"record":"cluster:scheduler_binding_latency_seconds:quantile"},{"alert":"K8SSchedulerDown","annotations":{"description":"There is no running K8S scheduler. New pods are not being assigned to nodes.","runbook":"https://coreos.com/tectonic/docs/latest/troubleshooting/controller-recovery.html#recovering-a-scheduler","summary":"Scheduler is down"},"expr":"absent(up{job=\"kube-scheduler\"} == 1)","for":"5m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}}]}]}}
    creationTimestamp: "2019-08-07T09:52:47Z"
    generation: 1
    labels:
      app: prometheus
      chart: exporter-kube-scheduler-0.1.9
      heritage: Tiller
      prometheus: kube-prometheus
      release: kube-prometheus
    name: kube-prometheus-exporter-kube-scheduler
    namespace: monitoring
    resourceVersion: "13922870"
    selfLink: /apis/monitoring.coreos.com/v1/namespaces/monitoring/prometheusrules/kube-prometheus-exporter-kube-scheduler
    uid: 1c539d06-b8f9-11e9-be96-f25fe3364e4a
  spec:
    groups:
    - name: kube-scheduler.rules
      rules:
      - expr: histogram_quantile(0.99, sum(scheduler_e2e_scheduling_latency_microseconds_bucket)
          BY (le, cluster)) / 1e+06
        labels:
          quantile: "0.99"
        record: cluster:scheduler_e2e_scheduling_latency_seconds:quantile
      - expr: histogram_quantile(0.9, sum(scheduler_e2e_scheduling_latency_microseconds_bucket)
          BY (le, cluster)) / 1e+06
        labels:
          quantile: "0.9"
        record: cluster:scheduler_e2e_scheduling_latency_seconds:quantile
      - expr: histogram_quantile(0.5, sum(scheduler_e2e_scheduling_latency_microseconds_bucket)
          BY (le, cluster)) / 1e+06
        labels:
          quantile: "0.5"
        record: cluster:scheduler_e2e_scheduling_latency_seconds:quantile
      - expr: histogram_quantile(0.99, sum(scheduler_scheduling_algorithm_latency_microseconds_bucket)
          BY (le, cluster)) / 1e+06
        labels:
          quantile: "0.99"
        record: cluster:scheduler_scheduling_algorithm_latency_seconds:quantile
      - expr: histogram_quantile(0.9, sum(scheduler_scheduling_algorithm_latency_microseconds_bucket)
          BY (le, cluster)) / 1e+06
        labels:
          quantile: "0.9"
        record: cluster:scheduler_scheduling_algorithm_latency_seconds:quantile
      - expr: histogram_quantile(0.5, sum(scheduler_scheduling_algorithm_latency_microseconds_bucket)
          BY (le, cluster)) / 1e+06
        labels:
          quantile: "0.5"
        record: cluster:scheduler_scheduling_algorithm_latency_seconds:quantile
      - expr: histogram_quantile(0.99, sum(scheduler_binding_latency_microseconds_bucket)
          BY (le, cluster)) / 1e+06
        labels:
          quantile: "0.99"
        record: cluster:scheduler_binding_latency_seconds:quantile
      - expr: histogram_quantile(0.9, sum(scheduler_binding_latency_microseconds_bucket)
          BY (le, cluster)) / 1e+06
        labels:
          quantile: "0.9"
        record: cluster:scheduler_binding_latency_seconds:quantile
      - expr: histogram_quantile(0.5, sum(scheduler_binding_latency_microseconds_bucket)
          BY (le, cluster)) / 1e+06
        labels:
          quantile: "0.5"
        record: cluster:scheduler_binding_latency_seconds:quantile
      - alert: K8SSchedulerDown
        annotations:
          description: There is no running K8S scheduler. New pods are not being assigned
            to nodes.
          runbook: https://coreos.com/tectonic/docs/latest/troubleshooting/controller-recovery.html#recovering-a-scheduler
          summary: Scheduler is down
        expr: absent(up{job="kube-scheduler"} == 1)
        for: 5m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
- apiVersion: monitoring.coreos.com/v1
  kind: PrometheusRule
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"monitoring.coreos.com/v1","kind":"PrometheusRule","metadata":{"annotations":{},"creationTimestamp":"2019-08-07T09:52:47Z","generation":1,"labels":{"app":"prometheus","chart":"exporter-kube-state-0.2.6","heritage":"Tiller","prometheus":"kube-prometheus","release":"kube-prometheus"},"name":"kube-prometheus-exporter-kube-state","namespace":"monitoring","resourceVersion":"101965","selfLink":"/apis/monitoring.coreos.com/v1/namespaces/monitoring/prometheusrules/kube-prometheus-exporter-kube-state","uid":"1c58e2f0-b8f9-11e9-be96-f25fe3364e4a"},"spec":{"groups":[{"name":"kube-state-metrics.rules","rules":[{"alert":"DeploymentGenerationMismatch","annotations":{"description":"Observed deployment generation does not match expected one for deployment {{$labels.namespace}}/{{$labels.deployment}}","summary":"Deployment is outdated"},"expr":"kube_deployment_status_observed_generation != kube_deployment_metadata_generation","for":"15m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"DeploymentReplicasNotUpdated","annotations":{"description":"Replicas are not updated and available for deployment {{$labels.namespace}}/{{$labels.deployment}}","summary":"Deployment replicas are outdated"},"expr":"((kube_deployment_status_replicas_updated != kube_deployment_spec_replicas) or (kube_deployment_status_replicas_available != kube_deployment_spec_replicas)) unless (kube_deployment_spec_paused == 1)","for":"15m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"DaemonSetRolloutStuck","annotations":{"description":"Only {{$value}}% of desired pods scheduled and ready for daemon set {{$labels.namespace}}/{{$labels.daemonset}}","summary":"DaemonSet is missing pods"},"expr":"kube_daemonset_status_number_ready / kube_daemonset_status_desired_number_scheduled * 100 \u003c 100","for":"15m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"K8SDaemonSetsNotScheduled","annotations":{"description":"A number of daemonsets are not scheduled.","summary":"Daemonsets are not scheduled correctly"},"expr":"kube_daemonset_status_desired_number_scheduled - kube_daemonset_status_current_number_scheduled \u003e 0","for":"10m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"DaemonSetsMissScheduled","annotations":{"description":"A number of daemonsets are running where they are not supposed to run.","summary":"Daemonsets are not scheduled correctly"},"expr":"kube_daemonset_status_number_misscheduled \u003e 0","for":"10m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"PodFrequentlyRestarting","annotations":{"description":"Pod {{$labels.namespace}}/{{$labels.pod}} was restarted {{$value}} times within the last hour","summary":"Pod is restarting frequently"},"expr":"increase(kube_pod_container_status_restarts_total[1h]) \u003e 5","for":"10m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}}]}]}}
    creationTimestamp: "2019-08-07T09:52:47Z"
    generation: 1
    labels:
      app: prometheus
      chart: exporter-kube-state-0.2.6
      heritage: Tiller
      prometheus: kube-prometheus
      release: kube-prometheus
    name: kube-prometheus-exporter-kube-state
    namespace: monitoring
    resourceVersion: "13922872"
    selfLink: /apis/monitoring.coreos.com/v1/namespaces/monitoring/prometheusrules/kube-prometheus-exporter-kube-state
    uid: 1c58e2f0-b8f9-11e9-be96-f25fe3364e4a
  spec:
    groups:
    - name: kube-state-metrics.rules
      rules:
      - alert: DeploymentGenerationMismatch
        annotations:
          description: Observed deployment generation does not match expected one
            for deployment {{$labels.namespace}}/{{$labels.deployment}}
          summary: Deployment is outdated
        expr: kube_deployment_status_observed_generation != kube_deployment_metadata_generation
        for: 15m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: DeploymentReplicasNotUpdated
        annotations:
          description: Replicas are not updated and available for deployment {{$labels.namespace}}/{{$labels.deployment}}
          summary: Deployment replicas are outdated
        expr: ((kube_deployment_status_replicas_updated != kube_deployment_spec_replicas)
          or (kube_deployment_status_replicas_available != kube_deployment_spec_replicas))
          unless (kube_deployment_spec_paused == 1)
        for: 15m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: DaemonSetRolloutStuck
        annotations:
          description: Only {{$value}}% of desired pods scheduled and ready for daemon
            set {{$labels.namespace}}/{{$labels.daemonset}}
          summary: DaemonSet is missing pods
        expr: kube_daemonset_status_number_ready / kube_daemonset_status_desired_number_scheduled
          * 100 < 100
        for: 15m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: K8SDaemonSetsNotScheduled
        annotations:
          description: A number of daemonsets are not scheduled.
          summary: Daemonsets are not scheduled correctly
        expr: kube_daemonset_status_desired_number_scheduled - kube_daemonset_status_current_number_scheduled
          > 0
        for: 10m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: DaemonSetsMissScheduled
        annotations:
          description: A number of daemonsets are running where they are not supposed
            to run.
          summary: Daemonsets are not scheduled correctly
        expr: kube_daemonset_status_number_misscheduled > 0
        for: 10m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: PodFrequentlyRestarting
        annotations:
          description: Pod {{$labels.namespace}}/{{$labels.pod}} was restarted {{$value}}
            times within the last hour
          summary: Pod is restarting frequently
        expr: increase(kube_pod_container_status_restarts_total[1h]) > 5
        for: 10m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
- apiVersion: monitoring.coreos.com/v1
  kind: PrometheusRule
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"monitoring.coreos.com/v1","kind":"PrometheusRule","metadata":{"annotations":{},"creationTimestamp":"2019-08-07T09:52:47Z","generation":1,"labels":{"app":"prometheus","chart":"exporter-kubelets-0.2.11","heritage":"Tiller","prometheus":"kube-prometheus","release":"kube-prometheus"},"name":"kube-prometheus-exporter-kubelets","namespace":"monitoring","resourceVersion":"101967","selfLink":"/apis/monitoring.coreos.com/v1/namespaces/monitoring/prometheusrules/kube-prometheus-exporter-kubelets","uid":"1c5c435c-b8f9-11e9-be96-f25fe3364e4a"},"spec":{"groups":[{"name":"kubelet.rules","rules":[{"alert":"K8SNodeNotReady","annotations":{"description":"The Kubelet on {{ $labels.node }} has not checked in with the API, or has set itself to NotReady, for more than an hour","summary":"Node status is NotReady"},"expr":"kube_node_status_condition{condition=\"Ready\",status=\"true\"} == 0","for":"1h","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"K8SManyNodesNotReady","annotations":{"description":"{{ $value }}% of Kubernetes nodes are not ready"},"expr":"count(kube_node_status_condition{condition=\"Ready\",status=\"true\"} == 0) \u003e 1 and (count(kube_node_status_condition{condition=\"Ready\",status=\"true\"} == 0) / count(kube_node_status_condition{condition=\"Ready\",status=\"true\"})) * 100 \u003e 20","for":"1m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}},{"alert":"K8SKubeletDown","annotations":{"description":"Prometheus failed to scrape {{ $value }}% of kubelets.","summary":"Prometheus failed to scrape"},"expr":"count(up{job=\"kubelet\"} == 0) / count(up{job=\"kubelet\"}) * 100 \u003e 3","for":"1h","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"K8SKubeletDown","annotations":{"description":"Prometheus failed to scrape {{ $value }}% of kubelets, or all Kubelets have disappeared from service discovery.","summary":"Many Kubelets cannot be scraped"},"expr":"(absent(up{job=\"kubelet\"} == 1) or count(up{job=\"kubelet\"} == 0) / count(up{job=\"kubelet\"})) * 100 \u003e 10","for":"1h","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}},{"alert":"K8SKubeletTooManyPods","annotations":{"description":"Kubelet {{$labels.instance}} is running {{$value}} pods, close to the limit of 110","summary":"Kubelet is close to pod limit"},"expr":"kubelet_running_pod_count \u003e 100","for":"10m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}}]}]}}
    creationTimestamp: "2019-08-07T09:52:47Z"
    generation: 1
    labels:
      app: prometheus
      chart: exporter-kubelets-0.2.11
      heritage: Tiller
      prometheus: kube-prometheus
      release: kube-prometheus
    name: kube-prometheus-exporter-kubelets
    namespace: monitoring
    resourceVersion: "13922875"
    selfLink: /apis/monitoring.coreos.com/v1/namespaces/monitoring/prometheusrules/kube-prometheus-exporter-kubelets
    uid: 1c5c435c-b8f9-11e9-be96-f25fe3364e4a
  spec:
    groups:
    - name: kubelet.rules
      rules:
      - alert: K8SNodeNotReady
        annotations:
          description: The Kubelet on {{ $labels.node }} has not checked in with the
            API, or has set itself to NotReady, for more than an hour
          summary: Node status is NotReady
        expr: kube_node_status_condition{condition="Ready",status="true"} == 0
        for: 1h
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: K8SManyNodesNotReady
        annotations:
          description: '{{ $value }}% of Kubernetes nodes are not ready'
        expr: count(kube_node_status_condition{condition="Ready",status="true"} ==
          0) > 1 and (count(kube_node_status_condition{condition="Ready",status="true"}
          == 0) / count(kube_node_status_condition{condition="Ready",status="true"}))
          * 100 > 20
        for: 1m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
      - alert: K8SKubeletDown
        annotations:
          description: Prometheus failed to scrape {{ $value }}% of kubelets.
          summary: Prometheus failed to scrape
        expr: count(up{job="kubelet"} == 0) / count(up{job="kubelet"}) * 100 > 3
        for: 1h
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: K8SKubeletDown
        annotations:
          description: Prometheus failed to scrape {{ $value }}% of kubelets, or all
            Kubelets have disappeared from service discovery.
          summary: Many Kubelets cannot be scraped
        expr: (absent(up{job="kubelet"} == 1) or count(up{job="kubelet"} == 0) / count(up{job="kubelet"}))
          * 100 > 10
        for: 1h
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
      - alert: K8SKubeletTooManyPods
        annotations:
          description: Kubelet {{$labels.instance}} is running {{$value}} pods, close
            to the limit of 110
          summary: Kubelet is close to pod limit
        expr: kubelet_running_pod_count > 100
        for: 10m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
- apiVersion: monitoring.coreos.com/v1
  kind: PrometheusRule
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"monitoring.coreos.com/v1","kind":"PrometheusRule","metadata":{"annotations":{},"creationTimestamp":"2019-08-07T09:52:47Z","generation":1,"labels":{"app":"prometheus","chart":"exporter-kubernetes-0.1.10","heritage":"Tiller","prometheus":"kube-prometheus","release":"kube-prometheus"},"name":"kube-prometheus-exporter-kubernetes","namespace":"monitoring","resourceVersion":"101971","selfLink":"/apis/monitoring.coreos.com/v1/namespaces/monitoring/prometheusrules/kube-prometheus-exporter-kubernetes","uid":"1c5f5bd4-b8f9-11e9-be96-f25fe3364e4a"},"spec":{"groups":[{"name":"kubernetes.rules","rules":[{"expr":"sum(container_memory_usage_bytes{container_name!=\"POD\",pod_name!=\"\"}) BY (pod_name)","record":"pod_name:container_memory_usage_bytes:sum"},{"expr":"sum(container_spec_cpu_shares{container_name!=\"POD\",pod_name!=\"\"}) BY (pod_name)","record":"pod_name:container_spec_cpu_shares:sum"},{"expr":"sum(rate(container_cpu_usage_seconds_total{container_name!=\"POD\",pod_name!=\"\"}[5m])) BY (pod_name)","record":"pod_name:container_cpu_usage:sum"},{"expr":"sum(container_fs_usage_bytes{container_name!=\"POD\",pod_name!=\"\"}) BY (pod_name)","record":"pod_name:container_fs_usage_bytes:sum"},{"expr":"sum(container_memory_usage_bytes{container_name!=\"\"}) BY (namespace)","record":"namespace:container_memory_usage_bytes:sum"},{"expr":"sum(container_spec_cpu_shares{container_name!=\"\"}) BY (namespace)","record":"namespace:container_spec_cpu_shares:sum"},{"expr":"sum(rate(container_cpu_usage_seconds_total{container_name!=\"POD\"}[5m])) BY (namespace)","record":"namespace:container_cpu_usage:sum"},{"expr":"sum(container_memory_usage_bytes{container_name!=\"POD\",pod_name!=\"\"}) BY (cluster) / sum(machine_memory_bytes) BY (cluster)","record":"cluster:memory_usage:ratio"},{"expr":"sum(container_spec_cpu_shares{container_name!=\"POD\",pod_name!=\"\"}) / 1000 / sum(machine_cpu_cores)","record":"cluster:container_spec_cpu_shares:ratio"},{"expr":"sum(rate(container_cpu_usage_seconds_total{container_name!=\"POD\",pod_name!=\"\"}[5m])) / sum(machine_cpu_cores)","record":"cluster:container_cpu_usage:ratio"},{"expr":"histogram_quantile(0.99, rate(apiserver_request_latencies_bucket[5m])) / 1e+06","labels":{"quantile":"0.99"},"record":"apiserver_latency_seconds:quantile"},{"expr":"histogram_quantile(0.9, rate(apiserver_request_latencies_bucket[5m])) / 1e+06","labels":{"quantile":"0.9"},"record":"apiserver_latency:quantile_seconds"},{"expr":"histogram_quantile(0.5, rate(apiserver_request_latencies_bucket[5m])) / 1e+06","labels":{"quantile":"0.5"},"record":"apiserver_latency_seconds:quantile"},{"alert":"APIServerLatencyHigh","annotations":{"description":"the API server has a 99th percentile latency of {{ $value }} seconds for {{$labels.verb}} {{$labels.resource}}","summary":"API server high latency"},"expr":"apiserver_latency_seconds:quantile{quantile=\"0.99\",subresource!=\"log\",verb!~\"^(?:WATCH|WATCHLIST|PROXY|CONNECT)$\"} \u003e 1","for":"10m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"APIServerLatencyHigh","annotations":{"description":"the API server has a 99th percentile latency of {{ $value }} seconds for {{$labels.verb}} {{$labels.resource}}","summary":"API server high latency"},"expr":"apiserver_latency_seconds:quantile{quantile=\"0.99\",subresource!=\"log\",verb!~\"^(?:WATCH|WATCHLIST|PROXY|CONNECT)$\"} \u003e 4","for":"10m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}},{"alert":"APIServerErrorsHigh","annotations":{"description":"API server returns errors for {{ $value }}% of requests","summary":"API server request errors"},"expr":"rate(apiserver_request_count{code=~\"^(?:5..)$\"}[5m]) / rate(apiserver_request_count[5m]) * 100 \u003e 2","for":"10m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"APIServerErrorsHigh","annotations":{"description":"API server returns errors for {{ $value }}% of requests"},"expr":"rate(apiserver_request_count{code=~\"^(?:5..)$\"}[5m]) / rate(apiserver_request_count[5m]) * 100 \u003e 5","for":"10m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}},{"alert":"K8SApiserverDown","annotations":{"description":"No API servers are reachable or all have disappeared from service discovery","summary":"No API servers are reachable"},"expr":"absent(up{job=\"apiserver\"} == 1)","for":"20m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}},{"alert":"K8sCertificateExpirationNotice","annotations":{"description":"Kubernetes API Certificate is expiring soon (less than 7 days)","summary":"Kubernetes API Certificate is expiering soon"},"expr":"sum(apiserver_client_certificate_expiration_seconds_bucket{le=\"604800\"}) \u003e 0","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"K8sCertificateExpirationNotice","annotations":{"description":"Kubernetes API Certificate is expiring in less than 1 day","summary":"Kubernetes API Certificate is expiering"},"expr":"sum(apiserver_client_certificate_expiration_seconds_bucket{le=\"86400\"}) \u003e 0","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}}]}]}}
    creationTimestamp: "2019-08-07T09:52:47Z"
    generation: 1
    labels:
      app: prometheus
      chart: exporter-kubernetes-0.1.10
      heritage: Tiller
      prometheus: kube-prometheus
      release: kube-prometheus
    name: kube-prometheus-exporter-kubernetes
    namespace: monitoring
    resourceVersion: "13922880"
    selfLink: /apis/monitoring.coreos.com/v1/namespaces/monitoring/prometheusrules/kube-prometheus-exporter-kubernetes
    uid: 1c5f5bd4-b8f9-11e9-be96-f25fe3364e4a
  spec:
    groups:
    - name: kubernetes.rules
      rules:
      - expr: sum(container_memory_usage_bytes{container_name!="POD",pod_name!=""})
          BY (pod_name)
        record: pod_name:container_memory_usage_bytes:sum
      - expr: sum(container_spec_cpu_shares{container_name!="POD",pod_name!=""}) BY
          (pod_name)
        record: pod_name:container_spec_cpu_shares:sum
      - expr: sum(rate(container_cpu_usage_seconds_total{container_name!="POD",pod_name!=""}[5m]))
          BY (pod_name)
        record: pod_name:container_cpu_usage:sum
      - expr: sum(container_fs_usage_bytes{container_name!="POD",pod_name!=""}) BY
          (pod_name)
        record: pod_name:container_fs_usage_bytes:sum
      - expr: sum(container_memory_usage_bytes{container_name!=""}) BY (namespace)
        record: namespace:container_memory_usage_bytes:sum
      - expr: sum(container_spec_cpu_shares{container_name!=""}) BY (namespace)
        record: namespace:container_spec_cpu_shares:sum
      - expr: sum(rate(container_cpu_usage_seconds_total{container_name!="POD"}[5m]))
          BY (namespace)
        record: namespace:container_cpu_usage:sum
      - expr: sum(container_memory_usage_bytes{container_name!="POD",pod_name!=""})
          BY (cluster) / sum(machine_memory_bytes) BY (cluster)
        record: cluster:memory_usage:ratio
      - expr: sum(container_spec_cpu_shares{container_name!="POD",pod_name!=""}) /
          1000 / sum(machine_cpu_cores)
        record: cluster:container_spec_cpu_shares:ratio
      - expr: sum(rate(container_cpu_usage_seconds_total{container_name!="POD",pod_name!=""}[5m]))
          / sum(machine_cpu_cores)
        record: cluster:container_cpu_usage:ratio
      - expr: histogram_quantile(0.99, rate(apiserver_request_latencies_bucket[5m]))
          / 1e+06
        labels:
          quantile: "0.99"
        record: apiserver_latency_seconds:quantile
      - expr: histogram_quantile(0.9, rate(apiserver_request_latencies_bucket[5m]))
          / 1e+06
        labels:
          quantile: "0.9"
        record: apiserver_latency:quantile_seconds
      - expr: histogram_quantile(0.5, rate(apiserver_request_latencies_bucket[5m]))
          / 1e+06
        labels:
          quantile: "0.5"
        record: apiserver_latency_seconds:quantile
      - alert: APIServerLatencyHigh
        annotations:
          description: the API server has a 99th percentile latency of {{ $value }}
            seconds for {{$labels.verb}} {{$labels.resource}}
          summary: API server high latency
        expr: apiserver_latency_seconds:quantile{quantile="0.99",subresource!="log",verb!~"^(?:WATCH|WATCHLIST|PROXY|CONNECT)$"}
          > 1
        for: 10m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: APIServerLatencyHigh
        annotations:
          description: the API server has a 99th percentile latency of {{ $value }}
            seconds for {{$labels.verb}} {{$labels.resource}}
          summary: API server high latency
        expr: apiserver_latency_seconds:quantile{quantile="0.99",subresource!="log",verb!~"^(?:WATCH|WATCHLIST|PROXY|CONNECT)$"}
          > 4
        for: 10m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
      - alert: APIServerErrorsHigh
        annotations:
          description: API server returns errors for {{ $value }}% of requests
          summary: API server request errors
        expr: rate(apiserver_request_count{code=~"^(?:5..)$"}[5m]) / rate(apiserver_request_count[5m])
          * 100 > 2
        for: 10m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: APIServerErrorsHigh
        annotations:
          description: API server returns errors for {{ $value }}% of requests
        expr: rate(apiserver_request_count{code=~"^(?:5..)$"}[5m]) / rate(apiserver_request_count[5m])
          * 100 > 5
        for: 10m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
      - alert: K8SApiserverDown
        annotations:
          description: No API servers are reachable or all have disappeared from service
            discovery
          summary: No API servers are reachable
        expr: absent(up{job="apiserver"} == 1)
        for: 20m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
      - alert: K8sCertificateExpirationNotice
        annotations:
          description: Kubernetes API Certificate is expiring soon (less than 7 days)
          summary: Kubernetes API Certificate is expiering soon
        expr: sum(apiserver_client_certificate_expiration_seconds_bucket{le="604800"})
          > 0
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: K8sCertificateExpirationNotice
        annotations:
          description: Kubernetes API Certificate is expiring in less than 1 day
          summary: Kubernetes API Certificate is expiering
        expr: sum(apiserver_client_certificate_expiration_seconds_bucket{le="86400"})
          > 0
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
- apiVersion: monitoring.coreos.com/v1
  kind: PrometheusRule
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"monitoring.coreos.com/v1","kind":"PrometheusRule","metadata":{"annotations":{},"creationTimestamp":"2019-08-07T09:52:47Z","generation":1,"labels":{"app":"prometheus","chart":"exporter-node-0.4.6","heritage":"Tiller","prometheus":"kube-prometheus","release":"kube-prometheus"},"name":"kube-prometheus-exporter-node","namespace":"monitoring","resourceVersion":"101974","selfLink":"/apis/monitoring.coreos.com/v1/namespaces/monitoring/prometheusrules/kube-prometheus-exporter-node","uid":"1c67b4fe-b8f9-11e9-be96-f25fe3364e4a"},"spec":{"groups":[{"name":"node.rules","rules":[{"expr":"sum(rate(node_cpu{mode!=\"idle\",mode!=\"iowait\"}[3m])) BY (instance)","record":"instance:node_cpu:rate:sum"},{"expr":"sum((node_filesystem_size{mountpoint=\"/\"} - node_filesystem_free{mountpoint=\"/\"})) BY (instance)","record":"instance:node_filesystem_usage:sum"},{"expr":"sum(rate(node_network_receive_bytes[3m])) BY (instance)","record":"instance:node_network_receive_bytes:rate:sum"},{"expr":"sum(rate(node_network_transmit_bytes[3m])) BY (instance)","record":"instance:node_network_transmit_bytes:rate:sum"},{"expr":"sum(rate(node_cpu{mode!=\"idle\",mode!=\"iowait\"}[5m])) WITHOUT (cpu, mode) / ON(instance) GROUP_LEFT() count(sum(node_cpu) BY (instance, cpu)) BY (instance)","record":"instance:node_cpu:ratio"},{"expr":"sum(rate(node_cpu{mode!=\"idle\",mode!=\"iowait\"}[5m]))","record":"cluster:node_cpu:sum_rate5m"},{"expr":"cluster:node_cpu:rate5m / count(sum(node_cpu) BY (instance, cpu))","record":"cluster:node_cpu:ratio"},{"alert":"NodeExporterDown","annotations":{"description":"Prometheus could not scrape a node-exporter for more than 10m, or node-exporters have disappeared from discovery","summary":"Prometheus could not scrape a node-exporter"},"expr":"absent(up{job=\"node-exporter\"} == 1)","for":"10m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"NodeDiskRunningFull","annotations":{"description":"device {{$labels.device}} on node {{$labels.instance}} is running full within the next 24 hours (mounted at {{$labels.mountpoint}})","summary":"Node disk is running full within 24 hours"},"expr":"predict_linear(node_filesystem_free{job=\"node-exporter\",mountpoint!~\"^/etc/(?:resolv.conf|hosts|hostname)$\"}[6h], 3600 * 24) \u003c 0 and on(instance) up{job=\"node-exporter\"}","for":"30m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"NodeDiskRunningFull","annotations":{"description":"device {{$labels.device}} on node {{$labels.instance}} is running full within the next 2 hours (mounted at {{$labels.mountpoint}})","summary":"Node disk is running full within 2 hours"},"expr":"predict_linear(node_filesystem_free{job=\"node-exporter\",mountpoint!~\"^/etc/(?:resolv.conf|hosts|hostname)$\"}[30m], 3600 * 2) \u003c 0 and on(instance) up{job=\"node-exporter\"}","for":"10m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}}]}]}}
    creationTimestamp: "2019-08-07T09:52:47Z"
    generation: 1
    labels:
      app: prometheus
      chart: exporter-node-0.4.6
      heritage: Tiller
      prometheus: kube-prometheus
      release: kube-prometheus
    name: kube-prometheus-exporter-node
    namespace: monitoring
    resourceVersion: "13922882"
    selfLink: /apis/monitoring.coreos.com/v1/namespaces/monitoring/prometheusrules/kube-prometheus-exporter-node
    uid: 1c67b4fe-b8f9-11e9-be96-f25fe3364e4a
  spec:
    groups:
    - name: node.rules
      rules:
      - expr: sum(rate(node_cpu{mode!="idle",mode!="iowait"}[3m])) BY (instance)
        record: instance:node_cpu:rate:sum
      - expr: sum((node_filesystem_size{mountpoint="/"} - node_filesystem_free{mountpoint="/"}))
          BY (instance)
        record: instance:node_filesystem_usage:sum
      - expr: sum(rate(node_network_receive_bytes[3m])) BY (instance)
        record: instance:node_network_receive_bytes:rate:sum
      - expr: sum(rate(node_network_transmit_bytes[3m])) BY (instance)
        record: instance:node_network_transmit_bytes:rate:sum
      - expr: sum(rate(node_cpu{mode!="idle",mode!="iowait"}[5m])) WITHOUT (cpu, mode)
          / ON(instance) GROUP_LEFT() count(sum(node_cpu) BY (instance, cpu)) BY (instance)
        record: instance:node_cpu:ratio
      - expr: sum(rate(node_cpu{mode!="idle",mode!="iowait"}[5m]))
        record: cluster:node_cpu:sum_rate5m
      - expr: cluster:node_cpu:rate5m / count(sum(node_cpu) BY (instance, cpu))
        record: cluster:node_cpu:ratio
      - alert: NodeExporterDown
        annotations:
          description: Prometheus could not scrape a node-exporter for more than 10m,
            or node-exporters have disappeared from discovery
          summary: Prometheus could not scrape a node-exporter
        expr: absent(up{job="node-exporter"} == 1)
        for: 10m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: NodeDiskRunningFull
        annotations:
          description: device {{$labels.device}} on node {{$labels.instance}} is running
            full within the next 24 hours (mounted at {{$labels.mountpoint}})
          summary: Node disk is running full within 24 hours
        expr: predict_linear(node_filesystem_free{job="node-exporter",mountpoint!~"^/etc/(?:resolv.conf|hosts|hostname)$"}[6h],
          3600 * 24) < 0 and on(instance) up{job="node-exporter"}
        for: 30m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: NodeDiskRunningFull
        annotations:
          description: device {{$labels.device}} on node {{$labels.instance}} is running
            full within the next 2 hours (mounted at {{$labels.mountpoint}})
          summary: Node disk is running full within 2 hours
        expr: predict_linear(node_filesystem_free{job="node-exporter",mountpoint!~"^/etc/(?:resolv.conf|hosts|hostname)$"}[30m],
          3600 * 2) < 0 and on(instance) up{job="node-exporter"}
        for: 10m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
- apiVersion: monitoring.coreos.com/v1
  kind: PrometheusRule
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"monitoring.coreos.com/v1","kind":"PrometheusRule","metadata":{"annotations":{},"creationTimestamp":"2019-08-07T09:52:47Z","generation":1,"labels":{"app":"prometheus","chart":"prometheus-0.0.51","heritage":"Tiller","prometheus":"kube-prometheus","release":"kube-prometheus"},"name":"kube-prometheus-rules","namespace":"monitoring","resourceVersion":"1779364","selfLink":"/apis/monitoring.coreos.com/v1/namespaces/monitoring/prometheusrules/kube-prometheus-rules","uid":"1c6d4ca5-b8f9-11e9-be96-f25fe3364e4a"},"spec":{"groups":[{"name":"prometheus.rules","rules":[{"alert":"PrometheusConfigReloadFailed","annotations":{"description":"Reloading Prometheus' configuration has failed for {{$labels.namespace}}/{{$labels.pod}}","summary":"Reloading Promehteus' configuration failed"},"expr":"prometheus_config_last_reload_successful == 0","for":"10m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"PrometheusNotificationQueueRunningFull","annotations":{"description":"Prometheus' alert notification queue is running full for {{$labels.namespace}}/{{ $labels.pod}}","summary":"Prometheus' alert notification queue is running full"},"expr":"predict_linear(prometheus_notifications_queue_length[5m], 60 * 30) \u003e prometheus_notifications_queue_capacity","for":"10m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"PrometheusErrorSendingAlerts","annotations":{"description":"Errors while sending alerts from Prometheus {{$labels.namespace}}/{{ $labels.pod}} to Alertmanager {{$labels.Alertmanager}}","summary":"Errors while sending alert from Prometheus"},"expr":"rate(prometheus_notifications_errors_total[5m]) / rate(prometheus_notifications_sent_total[5m]) \u003e 0.01","for":"10m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"PrometheusErrorSendingAlerts","annotations":{"description":"Errors while sending alerts from Prometheus {{$labels.namespace}}/{{ $labels.pod}} to Alertmanager {{$labels.Alertmanager}}","summary":"Errors while sending alerts from Prometheus"},"expr":"rate(prometheus_notifications_errors_total[5m]) / rate(prometheus_notifications_sent_total[5m]) \u003e 0.03","for":"10m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}},{"alert":"PrometheusNotConnectedToAlertmanagers","annotations":{"description":"Prometheus {{ $labels.namespace }}/{{ $labels.pod}} is not connected to any Alertmanagers","summary":"Prometheus is not connected to any Alertmanagers"},"expr":"prometheus_notifications_alertmanagers_discovered \u003c 1","for":"10m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"PrometheusTSDBReloadsFailing","annotations":{"description":"{{$labels.job}} at {{$labels.instance}} had {{$value | humanize}} reload failures over the last four hours.","summary":"Prometheus has issues reloading data blocks from disk"},"expr":"increase(prometheus_tsdb_reloads_failures_total[2h]) \u003e 0","for":"12h","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"PrometheusTSDBCompactionsFailing","annotations":{"description":"{{$labels.job}} at {{$labels.instance}} had {{$value | humanize}} compaction failures over the last four hours.","summary":"Prometheus has issues compacting sample blocks"},"expr":"increase(prometheus_tsdb_compactions_failed_total[2h]) \u003e 0","for":"12h","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"PrometheusTSDBWALCorruptions","annotations":{"description":"{{$labels.job}} at {{$labels.instance}} has a corrupted write-ahead log (WAL).","summary":"Prometheus write-ahead log is corrupted"},"expr":"tsdb_wal_corruptions_total \u003e 0","for":"4h","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"PrometheusNotIngestingSamples","annotations":{"description":"Prometheus {{ $labels.namespace }}/{{ $labels.pod}} isn't ingesting samples.","summary":"Prometheus isn't ingesting samples"},"expr":"rate(prometheus_tsdb_head_samples_appended_total[5m]) \u003c= 0","for":"10m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"PrometheusTargetScrapesDuplicate","annotations":{"description":"{{$labels.namespace}}/{{$labels.pod}} has many samples rejected due to duplicate timestamps but different values","summary":"Prometheus has many samples rejected"},"expr":"increase(prometheus_target_scrapes_sample_duplicate_timestamp_total[5m]) \u003e 0","for":"10m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}}]}]}}
    creationTimestamp: "2019-08-07T09:52:47Z"
    generation: 1
    labels:
      app: prometheus
      chart: prometheus-0.0.51
      heritage: Tiller
      prometheus: kube-prometheus
      release: kube-prometheus
    name: kube-prometheus-rules
    namespace: monitoring
    resourceVersion: "13922885"
    selfLink: /apis/monitoring.coreos.com/v1/namespaces/monitoring/prometheusrules/kube-prometheus-rules
    uid: 1c6d4ca5-b8f9-11e9-be96-f25fe3364e4a
  spec:
    groups:
    - name: prometheus.rules
      rules:
      - alert: PrometheusConfigReloadFailed
        annotations:
          description: Reloading Prometheus' configuration has failed for {{$labels.namespace}}/{{$labels.pod}}
          summary: Reloading Promehteus' configuration failed
        expr: prometheus_config_last_reload_successful == 0
        for: 10m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: PrometheusNotificationQueueRunningFull
        annotations:
          description: Prometheus' alert notification queue is running full for {{$labels.namespace}}/{{
            $labels.pod}}
          summary: Prometheus' alert notification queue is running full
        expr: predict_linear(prometheus_notifications_queue_length[5m], 60 * 30) >
          prometheus_notifications_queue_capacity
        for: 10m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: PrometheusErrorSendingAlerts
        annotations:
          description: Errors while sending alerts from Prometheus {{$labels.namespace}}/{{
            $labels.pod}} to Alertmanager {{$labels.Alertmanager}}
          summary: Errors while sending alert from Prometheus
        expr: rate(prometheus_notifications_errors_total[5m]) / rate(prometheus_notifications_sent_total[5m])
          > 0.01
        for: 10m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: PrometheusErrorSendingAlerts
        annotations:
          description: Errors while sending alerts from Prometheus {{$labels.namespace}}/{{
            $labels.pod}} to Alertmanager {{$labels.Alertmanager}}
          summary: Errors while sending alerts from Prometheus
        expr: rate(prometheus_notifications_errors_total[5m]) / rate(prometheus_notifications_sent_total[5m])
          > 0.03
        for: 10m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
      - alert: PrometheusNotConnectedToAlertmanagers
        annotations:
          description: Prometheus {{ $labels.namespace }}/{{ $labels.pod}} is not
            connected to any Alertmanagers
          summary: Prometheus is not connected to any Alertmanagers
        expr: prometheus_notifications_alertmanagers_discovered < 1
        for: 10m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: PrometheusTSDBReloadsFailing
        annotations:
          description: '{{$labels.job}} at {{$labels.instance}} had {{$value | humanize}}
            reload failures over the last four hours.'
          summary: Prometheus has issues reloading data blocks from disk
        expr: increase(prometheus_tsdb_reloads_failures_total[2h]) > 0
        for: 12h
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: PrometheusTSDBCompactionsFailing
        annotations:
          description: '{{$labels.job}} at {{$labels.instance}} had {{$value | humanize}}
            compaction failures over the last four hours.'
          summary: Prometheus has issues compacting sample blocks
        expr: increase(prometheus_tsdb_compactions_failed_total[2h]) > 0
        for: 12h
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: PrometheusTSDBWALCorruptions
        annotations:
          description: '{{$labels.job}} at {{$labels.instance}} has a corrupted write-ahead
            log (WAL).'
          summary: Prometheus write-ahead log is corrupted
        expr: tsdb_wal_corruptions_total > 0
        for: 4h
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: PrometheusNotIngestingSamples
        annotations:
          description: Prometheus {{ $labels.namespace }}/{{ $labels.pod}} isn't ingesting
            samples.
          summary: Prometheus isn't ingesting samples
        expr: rate(prometheus_tsdb_head_samples_appended_total[5m]) <= 0
        for: 10m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: PrometheusTargetScrapesDuplicate
        annotations:
          description: '{{$labels.namespace}}/{{$labels.pod}} has many samples rejected
            due to duplicate timestamps but different values'
          summary: Prometheus has many samples rejected
        expr: increase(prometheus_target_scrapes_sample_duplicate_timestamp_total[5m])
          > 0
        for: 10m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
- apiVersion: monitoring.coreos.com/v1
  kind: PrometheusRule
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"monitoring.coreos.com/v1","kind":"PrometheusRule","metadata":{"annotations":{},"creationTimestamp":"2019-08-16T08:52:21Z","generation":1,"labels":{"app":"prometheus","chart":"prometheus-0.0.51","heritage":"Tiller","prometheus":"kube-prometheus","release":"kube-prometheus","role":"alert-rules"},"name":"prometheus-example-rules","namespace":"monitoring","resourceVersion":"1915957","selfLink":"/apis/monitoring.coreos.com/v1/namespaces/monitoring/prometheusrules/prometheus-example-rules","uid":"28f1d3fa-c003-11e9-be96-f25fe3364e4a"},"spec":{"groups":[{"name":"./prom-example","rules":[{"alert":"Prometheus-example-HighRequestLatency","annotations":{"summary":"High request latency"},"expr":"job:request_latency_seconds:mean5m{job=\"kubelet\"} \u003e 0.5","for":"10m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"page"}}]},{"name":"./new-ocp41-clustermonitoring","rules":[{"alert":"new-ocp41-AlertmanagerDown","annotations":{"message":"Alertmanager has disappeared from Prometheus target discovery."},"expr":"absent(up{job=\"alertmanager-main\",namespace=\"monitoring\"} == 1)\n","for":"15m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}},{"alert":"new-ocp41-ClusterMonitoringOperatorDown","annotations":{"message":"ClusterMonitoringOperator has disappeared from Prometheus target discovery."},"expr":"absent(up{job=\"cluster-monitoring-operator\"} == 1)\n","for":"15m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}},{"alert":"new-ocp41-KubeAPIDown","annotations":{"message":"KubeAPI has disappeared from Prometheus target discovery."},"expr":"absent(up{job=\"apiserver\"} == 1)\n","for":"15m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}},{"alert":"new-ocp41-KubeControllerManagerDown","annotations":{"message":"KubeControllerManager has disappeared from Prometheus target discovery."},"expr":"absent(up{job=\"kube-controller-manager\"} == 1)\n","for":"15m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}},{"alert":"new-ocp41-KubeSchedulerDown","annotations":{"message":"KubeScheduler has disappeared from Prometheus target discovery."},"expr":"absent(up{job=\"scheduler\"} == 1)\n","for":"15m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}},{"alert":"new-ocp41-KubeStateMetricsDown","annotations":{"message":"KubeStateMetrics has disappeared from Prometheus target discovery."},"expr":"absent(up{job=\"kube-state-metrics\"} == 1)\n","for":"15m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}},{"alert":"new-ocp41-KubeletDown","annotations":{"message":"Kubelet has disappeared from Prometheus target discovery."},"expr":"absent(up{job=\"kubelet\"} == 1)\n","for":"15m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}},{"alert":"new-ocp41-NodeExporterDown","annotations":{"message":"NodeExporter has disappeared from Prometheus target discovery."},"expr":"absent(up{job=\"node-exporter\"} == 1)\n","for":"15m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}},{"alert":"new-ocp41-PrometheusDown","annotations":{"message":"Prometheus has disappeared from Prometheus target discovery."},"expr":"absent(up{job=\"prometheus-k8s\",namespace=\"monitoring\"} == 1)\n","for":"15m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}},{"alert":"new-ocp41-PrometheusOperatorDown","annotations":{"message":"PrometheusOperator has disappeared from Prometheus target discovery."},"expr":"absent(up{job=\"prometheus-operator\",namespace=\"monitoring\"} == 1)\n","for":"15m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}},{"alert":"new-ocp41-TelemeterClientDown","annotations":{"message":"TelemeterClient has disappeared from Prometheus target discovery."},"expr":"absent(up{job=\"telemeter-client\"} == 1)\n","for":"15m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}}]},{"name":"new-ocp41-kubernetes-apps","rules":[{"alert":"new-ocp41-KubePodCrashLooping","annotations":{"message":"Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is restarting {{ printf \"%.2f\" $value }} times / 5 minutes."},"expr":"rate(kube_pod_container_status_restarts_total{namespace=~\"(openshift-.*|kube-.*|default|logging)\",job=\"kube-state-metrics\"}[15m]) * 60 * 5 \u003e 0\n","for":"1h","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}},{"alert":"new-ocp41-KubePodNotReady","annotations":{"message":"Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready state for longer than an hour."},"expr":"sum by (namespace, pod) (kube_pod_status_phase{namespace=~\"(openshift-.*|kube-.*|default|logging)\",job=\"kube-state-metrics\", phase=~\"Pending|Unknown\"}) \u003e 0\n","for":"1h","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}},{"alert":"new-ocp41-KubeDeploymentGenerationMismatch","annotations":{"message":"Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment }} does not match, this indicates that the Deployment has failed but has not been rolled back."},"expr":"kube_deployment_status_observed_generation{namespace=~\"(openshift-.*|kube-.*|default|logging)\",job=\"kube-state-metrics\"}\n  !=\nkube_deployment_metadata_generation{namespace=~\"(openshift-.*|kube-.*|default|logging)\",job=\"kube-state-metrics\"}\n","for":"15m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}},{"alert":"new-ocp41-KubeDeploymentReplicasMismatch","annotations":{"message":"Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not matched the expected number of replicas for longer than an hour."},"expr":"kube_deployment_spec_replicas{namespace=~\"(openshift-.*|kube-.*|default|logging)\",job=\"kube-state-metrics\"}\n  !=\nkube_deployment_status_replicas_available{namespace=~\"(openshift-.*|kube-.*|default|logging)\",job=\"kube-state-metrics\"}\n","for":"1h","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}},{"alert":"new-ocp41-KubeStatefulSetReplicasMismatch","annotations":{"message":"StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has not matched the expected number of replicas for longer than 15 minutes."},"expr":"kube_statefulset_status_replicas_ready{namespace=~\"(openshift-.*|kube-.*|default|logging)\",job=\"kube-state-metrics\"}\n  !=\nkube_statefulset_status_replicas{namespace=~\"(openshift-.*|kube-.*|default|logging)\",job=\"kube-state-metrics\"}\n","for":"15m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}},{"alert":"new-ocp41-KubeStatefulSetGenerationMismatch","annotations":{"message":"StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset }} does not match, this indicates that the StatefulSet has failed but has not been rolled back."},"expr":"kube_statefulset_status_observed_generation{namespace=~\"(openshift-.*|kube-.*|default|logging)\",job=\"kube-state-metrics\"}\n  !=\nkube_statefulset_metadata_generation{namespace=~\"(openshift-.*|kube-.*|default|logging)\",job=\"kube-state-metrics\"}\n","for":"15m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}},{"alert":"new-ocp41-KubeStatefulSetUpdateNotRolledOut","annotations":{"message":"StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update has not been rolled out."},"expr":"max without (revision) (\n  kube_statefulset_status_current_revision{namespace=~\"(openshift-.*|kube-.*|default|logging)\",job=\"kube-state-metrics\"}\n    unless\n  kube_statefulset_status_update_revision{namespace=~\"(openshift-.*|kube-.*|default|logging)\",job=\"kube-state-metrics\"}\n)\n  *\n(\n  kube_statefulset_replicas{namespace=~\"(openshift-.*|kube-.*|default|logging)\",job=\"kube-state-metrics\"}\n    !=\n  kube_statefulset_status_replicas_updated{namespace=~\"(openshift-.*|kube-.*|default|logging)\",job=\"kube-state-metrics\"}\n)\n","for":"15m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}},{"alert":"new-ocp41-KubeDaemonSetRolloutStuck","annotations":{"message":"Only {{ $value }}% of the desired Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are scheduled and ready."},"expr":"kube_daemonset_status_number_ready{namespace=~\"(openshift-.*|kube-.*|default|logging)\",job=\"kube-state-metrics\"}\n  /\nkube_daemonset_status_desired_number_scheduled{namespace=~\"(openshift-.*|kube-.*|default|logging)\",job=\"kube-state-metrics\"} * 100 \u003c 100\n","for":"15m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}},{"alert":"new-ocp41-KubeDaemonSetNotScheduled","annotations":{"message":"{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are not scheduled."},"expr":"kube_daemonset_status_desired_number_scheduled{namespace=~\"(openshift-.*|kube-.*|default|logging)\",job=\"kube-state-metrics\"}\n  -\nkube_daemonset_status_current_number_scheduled{namespace=~\"(openshift-.*|kube-.*|default|logging)\",job=\"kube-state-metrics\"} \u003e 0\n","for":"10m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"new-ocp41-KubeDaemonSetMisScheduled","annotations":{"message":"{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are running where they are not supposed to run."},"expr":"kube_daemonset_status_number_misscheduled{namespace=~\"(openshift-.*|kube-.*|default|logging)\",job=\"kube-state-metrics\"} \u003e 0\n","for":"10m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"new-ocp41-KubeCronJobRunning","annotations":{"message":"CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is taking more than 1h to complete."},"expr":"time() - kube_cronjob_next_schedule_time{namespace=~\"(openshift-.*|kube-.*|default|logging)\",job=\"kube-state-metrics\"} \u003e 3600\n","for":"1h","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"new-ocp41-KubeJobCompletion","annotations":{"message":"Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking more than one hour to complete."},"expr":"kube_job_spec_completions{namespace=~\"(openshift-.*|kube-.*|default|logging)\",job=\"kube-state-metrics\"} - kube_job_status_succeeded{namespace=~\"(openshift-.*|kube-.*|default|logging)\",job=\"kube-state-metrics\"}  \u003e 0\n","for":"1h","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"new-ocp41-KubeJobFailed","annotations":{"message":"Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete."},"expr":"kube_job_status_failed{namespace=~\"(openshift-.*|kube-.*|default|logging)\",job=\"kube-state-metrics\"}  \u003e 0\n","for":"1h","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}}]},{"name":"new-ocp41-kubernetes-resources","rules":[{"alert":"new-ocp41-KubeCPUOvercommit","annotations":{"message":"Cluster has overcommitted CPU resource requests for Pods and cannot tolerate node failure."},"expr":"sum(namespace_name:kube_pod_container_resource_requests_cpu_cores:sum)\n  /\nsum(node:node_num_cpu:sum)\n  \u003e\n(count(node:node_num_cpu:sum)-1) / count(node:node_num_cpu:sum)\n","for":"5m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"new-ocp41-KubeMemOvercommit","annotations":{"message":"Cluster has overcommitted memory resource requests for Pods and cannot tolerate node failure."},"expr":"sum(namespace_name:kube_pod_container_resource_requests_memory_bytes:sum)\n  /\nsum(node_memory_MemTotal_bytes)\n  \u003e\n(count(node:node_num_cpu:sum)-1)\n  /\ncount(node:node_num_cpu:sum)\n","for":"5m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"new-ocp41-KubeCPUOvercommit","annotations":{"message":"Cluster has overcommitted CPU resource requests for Namespaces."},"expr":"sum(kube_resourcequota{namespace=~\"(openshift-.*|kube-.*|default|logging)\",job=\"kube-state-metrics\", type=\"hard\", resource=\"cpu\"})\n  /\nsum(node:node_num_cpu:sum)\n  \u003e 1.5\n","for":"5m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"new-ocp41-KubeMemOvercommit","annotations":{"message":"Cluster has overcommitted memory resource requests for Namespaces."},"expr":"sum(kube_resourcequota{namespace=~\"(openshift-.*|kube-.*|default|logging)\",job=\"kube-state-metrics\", type=\"hard\", resource=\"memory\"})\n  /\nsum(node_memory_MemTotal_bytes{job=\"node-exporter\"})\n  \u003e 1.5\n","for":"5m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"new-ocp41-KubeQuotaExceeded","annotations":{"message":"Namespace {{ $labels.namespace }} is using {{ printf \"%0.0f\" $value }}% of its {{ $labels.resource }} quota."},"expr":"100 * kube_resourcequota{namespace=~\"(openshift-.*|kube-.*|default|logging)\",job=\"kube-state-metrics\", type=\"used\"}\n  / ignoring(instance, job, type)\n(kube_resourcequota{namespace=~\"(openshift-.*|kube-.*|default|logging)\",job=\"kube-state-metrics\", type=\"hard\"} \u003e 0)\n  \u003e 90\n","for":"15m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"new-ocp41-CPUThrottlingHigh","annotations":{"message":"{{ printf \"%0.0f\" $value }}% throttling of CPU in namespace {{ $labels.namespace }} for container {{ $labels.container_name }} in pod {{ $labels.pod_name }}."},"expr":"100 * sum(increase(container_cpu_cfs_throttled_periods_total{container_name!=\"\", }[5m])) by (container_name, pod_name, namespace)\n  /\nsum(increase(container_cpu_cfs_periods_total{}[5m])) by (container_name, pod_name, namespace)\n  \u003e 25 \n","for":"15m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}}]},{"name":"new-ocp41-kubernetes-storage","rules":[{"alert":"new-ocp41-KubePersistentVolumeUsageCritical","annotations":{"message":"The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is only {{ printf \"%0.2f\" $value }}% free."},"expr":"100 * kubelet_volume_stats_available_bytes{namespace=~\"(openshift-.*|kube-.*|default|logging)\",job=\"kubelet\"}\n  /\nkubelet_volume_stats_capacity_bytes{namespace=~\"(openshift-.*|kube-.*|default|logging)\",job=\"kubelet\"}\n  \u003c 3\n","for":"1m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}},{"alert":"new-ocp41-KubePersistentVolumeFullInFourDays","annotations":{"message":"Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is expected to fill up within four days. Currently {{ printf \"%0.2f\" $value }}% is available."},"expr":"100 * (\n  kubelet_volume_stats_available_bytes{namespace=~\"(openshift-.*|kube-.*|default|logging)\",job=\"kubelet\"}\n    /\n  kubelet_volume_stats_capacity_bytes{namespace=~\"(openshift-.*|kube-.*|default|logging)\",job=\"kubelet\"}\n) \u003c 15\nand\npredict_linear(kubelet_volume_stats_available_bytes{namespace=~\"(openshift-.*|kube-.*|default|logging)\",job=\"kubelet\"}[6h], 4 * 24 * 3600) \u003c 0\n","for":"5m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}},{"alert":"new-ocp41-KubePersistentVolumeErrors","annotations":{"message":"The persistent volume {{ $labels.persistentvolume }} has status {{ $labels.phase }}."},"expr":"kube_persistentvolume_status_phase{phase=~\"Failed|Pending\",namespace=~\"(openshift-.*|kube-.*|default|logging)\",job=\"kube-state-metrics\"} \u003e 0\n","for":"5m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}}]},{"name":"new-ocp41-kubernetes-system","rules":[{"alert":"new-ocp41-KubeNodeNotReady","annotations":{"message":"{{ $labels.node }} has been unready for more than an hour."},"expr":"kube_node_status_condition{job=\"kube-state-metrics\",condition=\"Ready\",status=\"true\"} == 0\n","for":"1h","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"new-ocp41-KubeVersionMismatch","annotations":{"message":"There are {{ $value }} different semantic versions of Kubernetes components running."},"expr":"count(count by (gitVersion) (label_replace(kubernetes_build_info{job!=\"kube-dns\"},\"gitVersion\",\"$1\",\"gitVersion\",\"(v[0-9]*.[0-9]*.[0-9]*).*\"))) \u003e 1\n","for":"1h","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"new-ocp41-KubeClientErrors","annotations":{"message":"Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance }}' is experiencing {{ printf \"%0.0f\" $value }}% errors.'"},"expr":"(sum(rate(rest_client_requests_total{code=~\"5..\"}[5m])) by (instance, job)\n  /\nsum(rate(rest_client_requests_total[5m])) by (instance, job))\n* 100 \u003e 1\n","for":"15m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"new-ocp41-KubeClientErrors","annotations":{"message":"Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance }}' is experiencing {{ printf \"%0.0f\" $value }} errors / second."},"expr":"sum(rate(ksm_scrape_error_total{job=\"kube-state-metrics\"}[5m])) by (instance, job) \u003e 0.1\n","for":"15m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"new-ocp41-KubeletTooManyPods","annotations":{"message":"Kubelet {{ $labels.instance }} is running {{ $value }} Pods, close to the limit of 250."},"expr":"kubelet_running_pod_count{job=\"kubelet\"} \u003e 250 * 0.9\n","for":"15m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"new-ocp41-KubeAPILatencyHigh","annotations":{"message":"The API server has a 99th percentile latency of {{ $value }} seconds for {{ $labels.verb }} {{ $labels.resource }}."},"expr":"cluster_quantile:apiserver_request_latencies:histogram_quantile{job=\"apiserver\",quantile=\"0.99\",subresource!=\"log\",verb!~\"^(?:LIST|WATCH|WATCHLIST|PROXY|CONNECT)$\"} \u003e 1\n","for":"10m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"new-ocp41-KubeAPILatencyHigh","annotations":{"message":"The API server has a 99th percentile latency of {{ $value }} seconds for {{ $labels.verb }} {{ $labels.resource }}."},"expr":"cluster_quantile:apiserver_request_latencies:histogram_quantile{job=\"apiserver\",quantile=\"0.99\",subresource!=\"log\",verb!~\"^(?:LIST|WATCH|WATCHLIST|PROXY|CONNECT)$\"} \u003e 4\n","for":"10m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}},{"alert":"new-ocp41-KubeAPIErrorsHigh","annotations":{"message":"API server is returning errors for {{ $value }}% of requests."},"expr":"sum(rate(apiserver_request_count{job=\"apiserver\",code=~\"^(?:5..)$\"}[5m]))\n  /\nsum(rate(apiserver_request_count{job=\"apiserver\"}[5m])) * 100 \u003e 3\n","for":"10m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}},{"alert":"new-ocp41-KubeAPIErrorsHigh","annotations":{"message":"API server is returning errors for {{ $value }}% of requests."},"expr":"sum(rate(apiserver_request_count{job=\"apiserver\",code=~\"^(?:5..)$\"}[5m]))\n  /\nsum(rate(apiserver_request_count{job=\"apiserver\"}[5m])) * 100 \u003e 1\n","for":"10m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"new-ocp41-KubeAPIErrorsHigh","annotations":{"message":"API server is returning errors for {{ $value }}% of requests for {{ $labels.verb }} {{ $labels.resource }} {{ $labels.subresource }}."},"expr":"sum(rate(apiserver_request_count{job=\"apiserver\",code=~\"^(?:5..)$\"}[5m])) by (resource,subresource,verb)\n  /\nsum(rate(apiserver_request_count{job=\"apiserver\"}[5m])) by (resource,subresource,verb) * 100 \u003e 10\n","for":"10m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}},{"alert":"new-ocp41-KubeAPIErrorsHigh","annotations":{"message":"API server is returning errors for {{ $value }}% of requests for {{ $labels.verb }} {{ $labels.resource }} {{ $labels.subresource }}."},"expr":"sum(rate(apiserver_request_count{job=\"apiserver\",code=~\"^(?:5..)$\"}[5m])) by (resource,subresource,verb)\n  /\nsum(rate(apiserver_request_count{job=\"apiserver\"}[5m])) by (resource,subresource,verb) * 100 \u003e 5\n","for":"10m","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"new-ocp41-KubeClientCertificateExpiration","annotations":{"message":"A client certificate used to authenticate to the apiserver is expiring in less than 1.5 hours."},"expr":"apiserver_client_certificate_expiration_seconds_count{job=\"apiserver\"} \u003e 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job=\"apiserver\"}[5m]))) \u003c 5400\n","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"warning"}},{"alert":"new-ocp41-KubeClientCertificateExpiration","annotations":{"message":"A client certificate used to authenticate to the apiserver is expiring in less than 1.0 hours."},"expr":"apiserver_client_certificate_expiration_seconds_count{job=\"apiserver\"} \u003e 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job=\"apiserver\"}[5m]))) \u003c 3600\n","labels":{"customer":"CFEE_ABC","notification":"slack","severity":"critical"}}]}]}}
    creationTimestamp: "2019-08-16T08:52:21Z"
    generation: 1
    labels:
      app: prometheus
      chart: prometheus-0.0.51
      heritage: Tiller
      prometheus: kube-prometheus
      release: kube-prometheus
      role: alert-rules
    name: prometheus-example-rules
    namespace: monitoring
    resourceVersion: "13922890"
    selfLink: /apis/monitoring.coreos.com/v1/namespaces/monitoring/prometheusrules/prometheus-example-rules
    uid: 28f1d3fa-c003-11e9-be96-f25fe3364e4a
  spec:
    groups:
    - name: ./prom-example
      rules:
      - alert: Prometheus-example-HighRequestLatency
        annotations:
          summary: High request latency
        expr: job:request_latency_seconds:mean5m{job="kubelet"} > 0.5
        for: 10m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: page
    - name: ./new-ocp41-clustermonitoring
      rules:
      - alert: new-ocp41-AlertmanagerDown
        annotations:
          message: Alertmanager has disappeared from Prometheus target discovery.
        expr: |
          absent(up{job="alertmanager-main",namespace="monitoring"} == 1)
        for: 15m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
      - alert: new-ocp41-ClusterMonitoringOperatorDown
        annotations:
          message: ClusterMonitoringOperator has disappeared from Prometheus target
            discovery.
        expr: |
          absent(up{job="cluster-monitoring-operator"} == 1)
        for: 15m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
      - alert: new-ocp41-KubeAPIDown
        annotations:
          message: KubeAPI has disappeared from Prometheus target discovery.
        expr: |
          absent(up{job="apiserver"} == 1)
        for: 15m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
      - alert: new-ocp41-KubeControllerManagerDown
        annotations:
          message: KubeControllerManager has disappeared from Prometheus target discovery.
        expr: |
          absent(up{job="kube-controller-manager"} == 1)
        for: 15m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
      - alert: new-ocp41-KubeSchedulerDown
        annotations:
          message: KubeScheduler has disappeared from Prometheus target discovery.
        expr: |
          absent(up{job="scheduler"} == 1)
        for: 15m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
      - alert: new-ocp41-KubeStateMetricsDown
        annotations:
          message: KubeStateMetrics has disappeared from Prometheus target discovery.
        expr: |
          absent(up{job="kube-state-metrics"} == 1)
        for: 15m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
      - alert: new-ocp41-KubeletDown
        annotations:
          message: Kubelet has disappeared from Prometheus target discovery.
        expr: |
          absent(up{job="kubelet"} == 1)
        for: 15m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
      - alert: new-ocp41-NodeExporterDown
        annotations:
          message: NodeExporter has disappeared from Prometheus target discovery.
        expr: |
          absent(up{job="node-exporter"} == 1)
        for: 15m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
      - alert: new-ocp41-PrometheusDown
        annotations:
          message: Prometheus has disappeared from Prometheus target discovery.
        expr: |
          absent(up{job="prometheus-k8s",namespace="monitoring"} == 1)
        for: 15m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
      - alert: new-ocp41-PrometheusOperatorDown
        annotations:
          message: PrometheusOperator has disappeared from Prometheus target discovery.
        expr: |
          absent(up{job="prometheus-operator",namespace="monitoring"} == 1)
        for: 15m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
      - alert: new-ocp41-TelemeterClientDown
        annotations:
          message: TelemeterClient has disappeared from Prometheus target discovery.
        expr: |
          absent(up{job="telemeter-client"} == 1)
        for: 15m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
    - name: new-ocp41-kubernetes-apps
      rules:
      - alert: new-ocp41-KubePodCrashLooping
        annotations:
          message: Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container
            }}) is restarting {{ printf "%.2f" $value }} times / 5 minutes.
        expr: |
          rate(kube_pod_container_status_restarts_total{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}[15m]) * 60 * 5 > 0
        for: 1h
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
      - alert: new-ocp41-KubePodNotReady
        annotations:
          message: Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready
            state for longer than an hour.
        expr: |
          sum by (namespace, pod) (kube_pod_status_phase{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics", phase=~"Pending|Unknown"}) > 0
        for: 1h
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
      - alert: new-ocp41-KubeDeploymentGenerationMismatch
        annotations:
          message: Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment
            }} does not match, this indicates that the Deployment has failed but has
            not been rolled back.
        expr: |
          kube_deployment_status_observed_generation{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}
            !=
          kube_deployment_metadata_generation{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}
        for: 15m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
      - alert: new-ocp41-KubeDeploymentReplicasMismatch
        annotations:
          message: Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has
            not matched the expected number of replicas for longer than an hour.
        expr: |
          kube_deployment_spec_replicas{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}
            !=
          kube_deployment_status_replicas_available{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}
        for: 1h
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
      - alert: new-ocp41-KubeStatefulSetReplicasMismatch
        annotations:
          message: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has
            not matched the expected number of replicas for longer than 15 minutes.
        expr: |
          kube_statefulset_status_replicas_ready{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}
            !=
          kube_statefulset_status_replicas{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}
        for: 15m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
      - alert: new-ocp41-KubeStatefulSetGenerationMismatch
        annotations:
          message: StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset
            }} does not match, this indicates that the StatefulSet has failed but
            has not been rolled back.
        expr: |
          kube_statefulset_status_observed_generation{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}
            !=
          kube_statefulset_metadata_generation{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}
        for: 15m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
      - alert: new-ocp41-KubeStatefulSetUpdateNotRolledOut
        annotations:
          message: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update
            has not been rolled out.
        expr: |
          max without (revision) (
            kube_statefulset_status_current_revision{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}
              unless
            kube_statefulset_status_update_revision{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}
          )
            *
          (
            kube_statefulset_replicas{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}
              !=
            kube_statefulset_status_replicas_updated{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}
          )
        for: 15m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
      - alert: new-ocp41-KubeDaemonSetRolloutStuck
        annotations:
          message: Only {{ $value }}% of the desired Pods of DaemonSet {{ $labels.namespace
            }}/{{ $labels.daemonset }} are scheduled and ready.
        expr: |
          kube_daemonset_status_number_ready{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}
            /
          kube_daemonset_status_desired_number_scheduled{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"} * 100 < 100
        for: 15m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
      - alert: new-ocp41-KubeDaemonSetNotScheduled
        annotations:
          message: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
            }} are not scheduled.'
        expr: |
          kube_daemonset_status_desired_number_scheduled{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}
            -
          kube_daemonset_status_current_number_scheduled{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"} > 0
        for: 10m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: new-ocp41-KubeDaemonSetMisScheduled
        annotations:
          message: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
            }} are running where they are not supposed to run.'
        expr: |
          kube_daemonset_status_number_misscheduled{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"} > 0
        for: 10m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: new-ocp41-KubeCronJobRunning
        annotations:
          message: CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is taking
            more than 1h to complete.
        expr: |
          time() - kube_cronjob_next_schedule_time{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"} > 3600
        for: 1h
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: new-ocp41-KubeJobCompletion
        annotations:
          message: Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking more
            than one hour to complete.
        expr: |
          kube_job_spec_completions{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"} - kube_job_status_succeeded{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}  > 0
        for: 1h
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: new-ocp41-KubeJobFailed
        annotations:
          message: Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete.
        expr: |
          kube_job_status_failed{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}  > 0
        for: 1h
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
    - name: new-ocp41-kubernetes-resources
      rules:
      - alert: new-ocp41-KubeCPUOvercommit
        annotations:
          message: Cluster has overcommitted CPU resource requests for Pods and cannot
            tolerate node failure.
        expr: |
          sum(namespace_name:kube_pod_container_resource_requests_cpu_cores:sum)
            /
          sum(node:node_num_cpu:sum)
            >
          (count(node:node_num_cpu:sum)-1) / count(node:node_num_cpu:sum)
        for: 5m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: new-ocp41-KubeMemOvercommit
        annotations:
          message: Cluster has overcommitted memory resource requests for Pods and
            cannot tolerate node failure.
        expr: |
          sum(namespace_name:kube_pod_container_resource_requests_memory_bytes:sum)
            /
          sum(node_memory_MemTotal_bytes)
            >
          (count(node:node_num_cpu:sum)-1)
            /
          count(node:node_num_cpu:sum)
        for: 5m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: new-ocp41-KubeCPUOvercommit
        annotations:
          message: Cluster has overcommitted CPU resource requests for Namespaces.
        expr: |
          sum(kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics", type="hard", resource="cpu"})
            /
          sum(node:node_num_cpu:sum)
            > 1.5
        for: 5m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: new-ocp41-KubeMemOvercommit
        annotations:
          message: Cluster has overcommitted memory resource requests for Namespaces.
        expr: |
          sum(kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics", type="hard", resource="memory"})
            /
          sum(node_memory_MemTotal_bytes{job="node-exporter"})
            > 1.5
        for: 5m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: new-ocp41-KubeQuotaExceeded
        annotations:
          message: Namespace {{ $labels.namespace }} is using {{ printf "%0.0f" $value
            }}% of its {{ $labels.resource }} quota.
        expr: |
          100 * kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics", type="used"}
            / ignoring(instance, job, type)
          (kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics", type="hard"} > 0)
            > 90
        for: 15m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: new-ocp41-CPUThrottlingHigh
        annotations:
          message: '{{ printf "%0.0f" $value }}% throttling of CPU in namespace {{
            $labels.namespace }} for container {{ $labels.container_name }} in pod
            {{ $labels.pod_name }}.'
        expr: "100 * sum(increase(container_cpu_cfs_throttled_periods_total{container_name!=\"\",
          }[5m])) by (container_name, pod_name, namespace)\n  /\nsum(increase(container_cpu_cfs_periods_total{}[5m]))
          by (container_name, pod_name, namespace)\n  > 25 \n"
        for: 15m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
    - name: new-ocp41-kubernetes-storage
      rules:
      - alert: new-ocp41-KubePersistentVolumeUsageCritical
        annotations:
          message: The PersistentVolume claimed by {{ $labels.persistentvolumeclaim
            }} in Namespace {{ $labels.namespace }} is only {{ printf "%0.2f" $value
            }}% free.
        expr: |
          100 * kubelet_volume_stats_available_bytes{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kubelet"}
            /
          kubelet_volume_stats_capacity_bytes{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kubelet"}
            < 3
        for: 1m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
      - alert: new-ocp41-KubePersistentVolumeFullInFourDays
        annotations:
          message: Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim
            }} in Namespace {{ $labels.namespace }} is expected to fill up within
            four days. Currently {{ printf "%0.2f" $value }}% is available.
        expr: |
          100 * (
            kubelet_volume_stats_available_bytes{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kubelet"}
              /
            kubelet_volume_stats_capacity_bytes{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kubelet"}
          ) < 15
          and
          predict_linear(kubelet_volume_stats_available_bytes{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kubelet"}[6h], 4 * 24 * 3600) < 0
        for: 5m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
      - alert: new-ocp41-KubePersistentVolumeErrors
        annotations:
          message: The persistent volume {{ $labels.persistentvolume }} has status
            {{ $labels.phase }}.
        expr: |
          kube_persistentvolume_status_phase{phase=~"Failed|Pending",namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"} > 0
        for: 5m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
    - name: new-ocp41-kubernetes-system
      rules:
      - alert: new-ocp41-KubeNodeNotReady
        annotations:
          message: '{{ $labels.node }} has been unready for more than an hour.'
        expr: |
          kube_node_status_condition{job="kube-state-metrics",condition="Ready",status="true"} == 0
        for: 1h
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: new-ocp41-KubeVersionMismatch
        annotations:
          message: There are {{ $value }} different semantic versions of Kubernetes
            components running.
        expr: |
          count(count by (gitVersion) (label_replace(kubernetes_build_info{job!="kube-dns"},"gitVersion","$1","gitVersion","(v[0-9]*.[0-9]*.[0-9]*).*"))) > 1
        for: 1h
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: new-ocp41-KubeClientErrors
        annotations:
          message: Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance
            }}' is experiencing {{ printf "%0.0f" $value }}% errors.'
        expr: |
          (sum(rate(rest_client_requests_total{code=~"5.."}[5m])) by (instance, job)
            /
          sum(rate(rest_client_requests_total[5m])) by (instance, job))
          * 100 > 1
        for: 15m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: new-ocp41-KubeClientErrors
        annotations:
          message: Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance
            }}' is experiencing {{ printf "%0.0f" $value }} errors / second.
        expr: |
          sum(rate(ksm_scrape_error_total{job="kube-state-metrics"}[5m])) by (instance, job) > 0.1
        for: 15m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: new-ocp41-KubeletTooManyPods
        annotations:
          message: Kubelet {{ $labels.instance }} is running {{ $value }} Pods, close
            to the limit of 250.
        expr: |
          kubelet_running_pod_count{job="kubelet"} > 250 * 0.9
        for: 15m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: new-ocp41-KubeAPILatencyHigh
        annotations:
          message: The API server has a 99th percentile latency of {{ $value }} seconds
            for {{ $labels.verb }} {{ $labels.resource }}.
        expr: |
          cluster_quantile:apiserver_request_latencies:histogram_quantile{job="apiserver",quantile="0.99",subresource!="log",verb!~"^(?:LIST|WATCH|WATCHLIST|PROXY|CONNECT)$"} > 1
        for: 10m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: new-ocp41-KubeAPILatencyHigh
        annotations:
          message: The API server has a 99th percentile latency of {{ $value }} seconds
            for {{ $labels.verb }} {{ $labels.resource }}.
        expr: |
          cluster_quantile:apiserver_request_latencies:histogram_quantile{job="apiserver",quantile="0.99",subresource!="log",verb!~"^(?:LIST|WATCH|WATCHLIST|PROXY|CONNECT)$"} > 4
        for: 10m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
      - alert: new-ocp41-KubeAPIErrorsHigh
        annotations:
          message: API server is returning errors for {{ $value }}% of requests.
        expr: |
          sum(rate(apiserver_request_count{job="apiserver",code=~"^(?:5..)$"}[5m]))
            /
          sum(rate(apiserver_request_count{job="apiserver"}[5m])) * 100 > 3
        for: 10m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
      - alert: new-ocp41-KubeAPIErrorsHigh
        annotations:
          message: API server is returning errors for {{ $value }}% of requests.
        expr: |
          sum(rate(apiserver_request_count{job="apiserver",code=~"^(?:5..)$"}[5m]))
            /
          sum(rate(apiserver_request_count{job="apiserver"}[5m])) * 100 > 1
        for: 10m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: new-ocp41-KubeAPIErrorsHigh
        annotations:
          message: API server is returning errors for {{ $value }}% of requests for
            {{ $labels.verb }} {{ $labels.resource }} {{ $labels.subresource }}.
        expr: |
          sum(rate(apiserver_request_count{job="apiserver",code=~"^(?:5..)$"}[5m])) by (resource,subresource,verb)
            /
          sum(rate(apiserver_request_count{job="apiserver"}[5m])) by (resource,subresource,verb) * 100 > 10
        for: 10m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
      - alert: new-ocp41-KubeAPIErrorsHigh
        annotations:
          message: API server is returning errors for {{ $value }}% of requests for
            {{ $labels.verb }} {{ $labels.resource }} {{ $labels.subresource }}.
        expr: |
          sum(rate(apiserver_request_count{job="apiserver",code=~"^(?:5..)$"}[5m])) by (resource,subresource,verb)
            /
          sum(rate(apiserver_request_count{job="apiserver"}[5m])) by (resource,subresource,verb) * 100 > 5
        for: 10m
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: new-ocp41-KubeClientCertificateExpiration
        annotations:
          message: A client certificate used to authenticate to the apiserver is expiring
            in less than 1.5 hours.
        expr: |
          apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 5400
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: warning
      - alert: new-ocp41-KubeClientCertificateExpiration
        annotations:
          message: A client certificate used to authenticate to the apiserver is expiring
            in less than 1.0 hours.
        expr: |
          apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 3600
        labels:
          customer: CFEE_ABC
          notification: slack
          severity: critical
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
